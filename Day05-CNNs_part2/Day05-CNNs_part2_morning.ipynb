{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YehS8enAmDn"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1DvKhAzLtk-Hilu7Le73WAOz2EBR5d41G\" width=\"500\"/>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CY6juJtSkmD"
      },
      "source": [
        "# **CNNs: convolutional neural networks (part 2)**\n",
        "\n",
        "#### **Morning contents/agenda**\n",
        "\n",
        "1. Commonly used datasets in computer vision\n",
        "\n",
        "2. Important CNN architectures\n",
        "\n",
        "3. U-nets and upsampling (unpooling & transpose convolutions)\n",
        "\n",
        "4. Transfer learning\n",
        "\n",
        "5. Summary of CNNs\n",
        "\n",
        "#### **Learning outcomes**\n",
        "\n",
        "1. Awareness of well-established CNN architectures\n",
        "\n",
        "2. Undersand how to upsample data\n",
        "\n",
        "3. Understand how and why transfer learning is used\n",
        "\n",
        "#### **Afternoon contents/agenda**\n",
        "\n",
        "1. Inspection of CNN filters\n",
        "\n",
        "2. Transfer learning from ImageNet to Bees and Ants\n",
        "\n",
        "#### **Learning outcomes**\n",
        "\n",
        "1. Become familiar with the effect that filters have (sometimes you can interpret them, sometimes they have abstracted the data too far to develop intuitions)\n",
        "\n",
        "2. Hands-on knowledge on how to apply transfer learning\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "---\n",
        "\n",
        "<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kYki018eTFJ",
        "outputId": "83c256bc-8cc3-4932-b10b-f8f4bba6e977"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pycm in /Users/shiyunwa/anaconda3/envs/deeplearning/lib/python3.9/site-packages (4.0)\n",
            "Requirement already satisfied: livelossplot in /Users/shiyunwa/anaconda3/envs/deeplearning/lib/python3.9/site-packages (0.5.5)\n",
            "Requirement already satisfied: art>=1.8 in /Users/shiyunwa/anaconda3/envs/deeplearning/lib/python3.9/site-packages (from pycm) (6.1)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /Users/shiyunwa/anaconda3/envs/deeplearning/lib/python3.9/site-packages (from pycm) (1.24.1)\n",
            "Requirement already satisfied: matplotlib in /Users/shiyunwa/anaconda3/envs/deeplearning/lib/python3.9/site-packages (from livelossplot) (3.8.2)\n",
            "Requirement already satisfied: bokeh in /Users/shiyunwa/anaconda3/envs/deeplearning/lib/python3.9/site-packages (from livelossplot) (3.3.1)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /Users/shiyunwa/anaconda3/envs/deeplearning/lib/python3.9/site-packages (from bokeh->livelossplot) (3.1.2)\n",
            "Requirement already satisfied: contourpy>=1 in /Users/shiyunwa/anaconda3/envs/deeplearning/lib/python3.9/site-packages (from bokeh->livelossplot) (1.2.0)\n",
            "Requirement already satisfied: packaging>=16.8 in /Users/shiyunwa/anaconda3/envs/deeplearning/lib/python3.9/site-packages (from bokeh->livelossplot) (23.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /Users/shiyunwa/anaconda3/envs/deeplearning/lib/python3.9/site-packages (from bokeh->livelossplot) (2.1.3)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /Users/shiyunwa/anaconda3/envs/deeplearning/lib/python3.9/site-packages (from bokeh->livelossplot) (9.3.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /Users/shiyunwa/anaconda3/envs/deeplearning/lib/python3.9/site-packages (from bokeh->livelossplot) (6.0.1)\n",
            "Requirement already satisfied: tornado>=5.1 in /Users/shiyunwa/anaconda3/envs/deeplearning/lib/python3.9/site-packages (from bokeh->livelossplot) (6.1)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /Users/shiyunwa/anaconda3/envs/deeplearning/lib/python3.9/site-packages (from bokeh->livelossplot) (2023.10.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/shiyunwa/anaconda3/envs/deeplearning/lib/python3.9/site-packages (from matplotlib->livelossplot) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/shiyunwa/anaconda3/envs/deeplearning/lib/python3.9/site-packages (from matplotlib->livelossplot) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/shiyunwa/anaconda3/envs/deeplearning/lib/python3.9/site-packages (from matplotlib->livelossplot) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/shiyunwa/anaconda3/envs/deeplearning/lib/python3.9/site-packages (from matplotlib->livelossplot) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/shiyunwa/anaconda3/envs/deeplearning/lib/python3.9/site-packages (from matplotlib->livelossplot) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/shiyunwa/anaconda3/envs/deeplearning/lib/python3.9/site-packages (from matplotlib->livelossplot) (6.1.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /Users/shiyunwa/anaconda3/envs/deeplearning/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->livelossplot) (3.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/shiyunwa/anaconda3/envs/deeplearning/lib/python3.9/site-packages (from Jinja2>=2.9->bokeh->livelossplot) (2.1.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/shiyunwa/anaconda3/envs/deeplearning/lib/python3.9/site-packages (from pandas>=1.2->bokeh->livelossplot) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/shiyunwa/anaconda3/envs/deeplearning/lib/python3.9/site-packages (from pandas>=1.2->bokeh->livelossplot) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /Users/shiyunwa/anaconda3/envs/deeplearning/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->livelossplot) (1.16.0)\n",
            "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ],
      "source": [
        "!pip install pycm livelossplot\n",
        "%pylab inline\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "from livelossplot import PlotLosses\n",
        "from pycm import *\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI8sNA9feT3H",
        "outputId": "a33995c5-cef7-435e-e39d-2bf4a6a64766"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No GPU available!\n"
          ]
        }
      ],
      "source": [
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
        "    torch.backends.cudnn.enabled   = False\n",
        "\n",
        "    return True\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
        "    print(\"Cuda installed! Running on GPU!\")\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"No GPU available!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVjTNdKHVF0H"
      },
      "source": [
        "## 1. Commonly used datasets in computer vision\n",
        "\n",
        "As we saw on the first week, the network capacity has to be adjusted in order to avoid overfitting to the data. In other words, very deep networks with large number of trainable parameters require big datasets because they have a lot of capacity to accomodate variations in the data.\n",
        "\n",
        "So far we have seen MNIST and similarly-smalled sized datasets:\n",
        "\n",
        "<br>\n",
        "\n",
        "<p align = \"center\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\" width=\"400\"/></p><p align = \"center\">\n",
        "<i>MNIST dataset: 60k training & 10k test images</i>\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "It is often desirable to have datasets of natural images, as they can be used for a broader range of applications than MNIST-like datasets. [CIFAR-10 and CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) are two datasets of natural images with 10 and 100 classes respectively:\n",
        "\n",
        "<br>\n",
        "\n",
        "<p align = \"center\"><img src=\"https://production-media.paperswithcode.com/datasets/4fdf2b82-2bc3-4f97-ba51-400322b228b1.png\" width=\"400\"/></p><p align = \"center\">\n",
        "<i>CIFAR-10 dataset: 50k training & 10k test images</i>\n",
        "</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "<p align = \"center\"><img src=\"https://miro.medium.com/max/1400/0*fqFMfJeP6CuBTuYc.webp\" width=\"400\"/></p><p align = \"center\">\n",
        "<i>CIFAR-100 dataset: 50k training & 10k test images</i>\n",
        "</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "But larger datasets exist as well. [ImageNet](https://www.image-net.org/) has been used in various competitions, and it contains more than 14 million images and 20k classes:\n",
        "\n",
        "<p align = \"center\"><img src=\"https://i0.wp.com/syncedreview.com/wp-content/uploads/2020/06/Imagenet.jpg?fit=1400%2C600&ssl=1\" width=\"800\"/></p><p align = \"center\">\n",
        "<i>ImageNet: >14M images and 20k classes </i>\n",
        "</p>\n",
        "\n",
        "\n",
        "[Here](https://pytorch.org/vision/stable/datasets.html) is a list of available datasets in `torchvision.datasets`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sinIebTtMUvs"
      },
      "source": [
        "## 2. Important CNN architectures\n",
        "\n",
        "Since their introduction in 1998 with LeNet-5, convolutional neural networks have evolved significantly and competed for the top spot in computer vision tasks.\n",
        "\n",
        "You can find a [good overview here](https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d) (the images below are from this website).\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1BGcWnSRGLJmVzfRSQBujnkHtF9wTeQuq\" width=\"600\"/>\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1PDNr20s96ddbabkX5dfjnUemw2ZMWvU4\" width=\"800\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1FycJ5amqUL-Z_NXKtfmbejqID6pTFSsQ\" width=\"800\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1yrpMY7PuyVG68M6gsnQhEHdGCbMZSYzo\" width=\"800\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=16TLC9m8JvZw1V5fVx3cqzDowfGk4TKUT\" width=\"800\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=178gJwbpE12TzKbHQfq3q8CTg4X-12gSZ\" width=\"800\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=14tHV7AMln-qH4DwpjDZUbSrj6o0Jx1YX\" width=\"800\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "As you can see, network sizes increase over time thanks to advances in computational power (better GPUs with more memory, etc):\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1Bv0msGB95GXeiuKs5GQI6TMHYQQljz1z\" width=\"600\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "But even this numbers are considered small in modern architectures. For example, **GPT-4** has 1.7 trillion parameters (largest network so far, I think).\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPC2aDTe83n_"
      },
      "source": [
        "## 2. U-nets and upsampling (unpooling & transpose convolutions)\n",
        "\n",
        "What are the outputs of the CNNs we have seen so far?\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1xSZ3Tb6mJgHit51fEZF9P5UojA0Tossm\" width=\"600\"/></center>\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAWL3EvbUEOV"
      },
      "source": [
        "<br>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1EITH6oofcQurxnnXNKriZ1dBkWmerlTX\" width=\"600\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "But CNNs have other applications. In the field of computer vision, a very common architecture is the **U-Net** which is a type of convolutional autoencoder (we will see autoencoders next week):\n",
        "\n",
        "<p align = \"center\"><img src=\"https://drive.google.com/uc?id=1C5oJBIihVeyditn1I0nIjlZfEpkIhH2F\" width=\"800\"/></p><p align = \"center\">\n",
        "<i> sources: <a href=\"https://arxiv.org/pdf/1505.04597.pdf\">original unet</a>, <a href=\"https://www.kaggle.com/c/tgs-salt-identification-challenge\"> seismic segmentation</a></i>\n",
        "</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "An important operation we perform to generate U-Net (and other architectures) is upscaling. The most common methods are:\n",
        "\n",
        "- nearest neighbour\n",
        "- \"bed of nails\"\n",
        "- Max unpooling\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1b1-3ncTi9cO7XFFuFljvfkwFlhKFHltU\" width=\"800\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "and\n",
        "\n",
        "- Transposed convolution or up-convolution, but **not deconvolution!**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urjrxKFIRF_2"
      },
      "source": [
        "## Transposed convolutions\n",
        "\n",
        "Transposed convolutions can be computed by following an easy recipe:\n",
        "\n",
        "<center><img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*54-7typHLLXhdvAhlku9SQ.png\" width=\"900\"/></center>\n",
        "\n",
        "\n",
        "where we know that:\n",
        "- `s`: stride\n",
        "- `p`: padding\n",
        "- `k`: kernel size\n",
        "\n",
        "and we use this hyperparameters to calculate:\n",
        "- `z`: how many zeros to insert in between pixels of my input\n",
        "- `p'`: how much padding do I add around the image\n",
        "\n",
        "But with the added caveat that, **as the name indicates**, we need to **transpose the kernel** before using it to convolve with the input.\n",
        "\n",
        "# Exercise:\n",
        "Let's practice with a couple of examples. First, let's try and calculate a simple case by hand:\n",
        "\n",
        "\n",
        "<p align = \"center\"><img src=\"https://drive.google.com/uc?id=1KPteXRKw7OwUKzZkO95MvGAt1qnm-b51\" width=\"800\"/></p><p align = \"center\">\n",
        "\n",
        "\n",
        "To check if we have the right solution we can use [`conv_transpose2d`](https://pytorch.org/docs/stable/generated/torch.nn.functional.conv_transpose2d.html), which allows us to pass an input and a predefined filter, whereas the [`ConvTranspose2d`](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d) layer randomly initialises the weights of the kernel which is not what we need now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[0, 0, 0, 0]\n",
        "[0, 1, 5, 0]\n",
        "[0, 2, 3, 0]\n",
        "[0, 0, 0, 0]\n",
        "\n",
        "[3, 2]\n",
        "[1, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qU5o6oetS_qo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1, 2, 2])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[[[ 0.,  1.,  5.],\n",
              "          [ 2., 15., 18.],\n",
              "          [ 4., 12.,  9.]]]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.tensor([[[[1, 5.], [2., 3.]]]])          ## why do I need to get an instance as below?\n",
        "#x = torch.tensor([[[[1, 5.], [2., 3.]]]])     ## how many square brackets do I need, and what do they do?\n",
        "print(x.shape)\n",
        "\n",
        "kernel = torch.tensor([[[[0, 1.], [2., 3.]]]])\n",
        "# kernel = torch.tensor([[[[0, 1.], [2., 3.]]]])\n",
        "\n",
        "torch.nn.functional.conv_transpose2d(x,kernel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3FyeCyaRjuo"
      },
      "source": [
        "Transpose convolutions, particularly with stride bigger than 1 can lead to checkerboard imprints on the outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "wXcJzNfOmFAG",
        "outputId": "f5945aa5-a3af-4034-8ec2-5c1dc522411e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe src=\"https://distill.pub/2016/deconv-checkerboard/\" width=\"1000\" height=\"500\"></iframe>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%html\n",
        "<iframe src=\"https://distill.pub/2016/deconv-checkerboard/\" width=\"1000\" height=\"500\"></iframe>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LzLdZmAdnPd"
      },
      "source": [
        "## 3. Transfer learning\n",
        "\n",
        "What is transfer learning and why is it useful? A definition from the Deep Learning book by Goodfellow et al (2016):\n",
        "\n",
        "*Transfer learning and domain adaptation refer to the situation where what has been learned in one setting ... is exploited to improve generalization in another setting.*\n",
        "\n",
        "<br>\n",
        "\n",
        "- The most well-known CNN designs are **available** on-line and have been **successfully trained** on very large number of images (millions).\n",
        "\n",
        "- In many applications we often work with a relatively **small number of images** (or data in general).\n",
        "\n",
        "- The idea of transfer learning is to use an existing trained CNN model which tries to solve a problem of similar nature and **tailor the model** to our particular application.\n",
        "\n",
        "The two main strategies are:\n",
        "\n",
        "1. **Add one (or more) layers, or retrain the last layer(s) of a pre-trained network**: This strategy assumes that the filters of most of the network do a good job at extracting data features we can use. The last layers, then, act as a final fine-tunning to capture the specific features of our data.\n",
        "\n",
        "2. **Retrain the whole network with small learning rates:** This strategy assumes that as a whole, the network captures data features well, and it only needs a bit of a ‘nudge’ to adapt the network parameters to our particular problem. In this case, we want to keep the underlying abstraction that the network does at different scales, but fine-tune it to our problem.\n",
        "\n",
        "We will see examples of both in this afternoon exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKJiEBxrgHc3"
      },
      "source": [
        "## 4. Receptive field\n",
        "\n",
        "The receptive field is defined as the region of the input that affects the output, and it can be defined between adjacent or non-adjacent CNN layers.\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1GfG26m6Xee9qhyiA-ARvbTnzAxbSYo--\" width=\"800\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "Why are we interested in the receptive field in our network?\n",
        "\n",
        "Because the receptive field will determine what are the hyperparameters\n",
        "I need to ensure full receptive field on my inputs:\n",
        "\n",
        "- filter size and stride\n",
        "- number of convolutional layers in the network\n",
        "\n",
        "[Here](https://www.baeldung.com/cs/cnn-receptive-field-size) you can find a more detailed explanation with a few formulas to compute receptive fields."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yXL0BFKZglc"
      },
      "source": [
        "# solution exercise transpose convolution:\n",
        "\n",
        "\n",
        "\n",
        "<p align = \"center\"><img src=\"https://drive.google.com/uc?id=1KPteXRKw7OwUKzZkO95MvGAt1qnm-b51\" width=\"800\"/></p><p align = \"center\">\n",
        "\n",
        "<p align = \"center\"><img src=\"https://drive.google.com/uc?id=1Q56nHt_tT6L7YOoNBH7PHduS4CrO41o4\" width=\"800\"/></p><p align = \"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHuxr6YEhRb2"
      },
      "source": [
        "And check that we can get the same result with a normal convolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mCfb6buxhR8k"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[ 0.,  1.,  5.],\n",
              "          [ 2., 15., 18.],\n",
              "          [ 4., 12.,  9.]]]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#x = torch.tensor([[1, 5.], [3., 2.]])          ## why do I need to get an instance as below?\n",
        "x = torch.tensor([[[[1, 5.], [2., 3.]]]])     ## how many square brackets do I need, and what do they do?\n",
        "#print(x.shape)\n",
        "\n",
        "kernel = torch.tensor([[[[3, 2.], [1., 0.]]]]) ## transpose the kernel here since we will use an 'normal' convolution\n",
        "\n",
        "torch.nn.functional.conv2d(x, kernel, padding=1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
